<!--
 * @Author: Zihui Cheng
 * @Description: 
 * 
-->
<p align="center">
<h1 align="center"> <img src="imgs/title.png" alt="SVG Image" width="40px"> CoMT: A Novel Benchmark for Chain of Multi-modal Thought on Large Vision-Language Models</h1>
</p>
<p align="center">
  	<a href="https://img.shields.io/badge/version-v0.0.1-blue">
      <img alt="version" src="https://img.shields.io/badge/version-v0.0.1-blue?color=FF8000?color=009922" />
    </a>
    <a >
       <img alt="PRs-Welcome" src="https://img.shields.io/badge/PRs-Welcome-blue" />
  	</a>
   	<a href="https://github.com/LightChen233/M3CoT/stargazers">
       <img alt="stars" src="https://img.shields.io/github/stars/LightChen233/M3CoT" />
  	</a>
  	<a href="https://github.com/LightChen233/M3CoT/network/members">
       <img alt="FORK" src="https://img.shields.io/github/forks/LightChen233/M3CoT?color=FF8000" />
  	</a>
    <a href="https://github.com/LightChen233/M3CoT/issues">
      <img alt="Issues" src="https://img.shields.io/github/issues/LightChen233/M3CoT?color=0088ff"/>
    </a>
    <br />
</p>

<p align="center">
  	<b>
    | [<a href="https://arxiv.org/abs/2410.05695">ArXiv</a>] | [<a href="https://huggingface.co/datasets/LightChen2333/BigGSM">ğŸ¤—HuggingFace</a>] |
    </b>
    <br />
</p>

ğŸŒŸ Any contributions via PRs, issues, emails or other methods are greatly appreciated.

## ğŸ”¥News
- ğŸ–ï¸ **Our work is accepted by AAAI 2025 !**
- ğŸ”¥ **We have release benchmark on \[[ğŸ¤—HuggingFace](https://huggingface.co/datasets/LightChen2333/BigGSM)\].**
- ğŸ”¥ **The paper is also available on \[[ArXiv](https://arxiv.org/abs/2410.05695)\].**

